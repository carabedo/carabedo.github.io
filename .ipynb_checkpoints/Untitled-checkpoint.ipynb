{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "## Topic Modelling\n",
    "\n",
    "\n",
    "Mas alla de la representacion de las palabras, podemos pensar y querer clasificar de que hablan las palabras. A esto se lo denomina Topic Modeling es encontrar los temas de los que habla un documento.\n",
    "\n",
    "\n",
    "Topic Modeling se puede utilizar para:\n",
    "\n",
    "* Dimensionality Reduction (Matrices de BoW son de alta dimensionalidad y estan llenas de nulos)\n",
    "* Unsupervised Learning (Exploracion de patrones)\n",
    "* Tagging (Asociar etiquetas a documentos)\n",
    "\n",
    "\n",
    "\n",
    "# Reduccion Dimensionalidad\n",
    "\n",
    "\n",
    "## LSA\n",
    "\n",
    "Latent Semantic Analysis (LSA) es una técnica de procesamiento de lenguaje natural desarrollada por Jerome Bellegarda en 2004. El objetivo de LSA es reducir la dimensión a partir de la descomposición de valores singulares (SVD) de una matriz BoW. La idea es que las palabras aparecerán en partes de texto similares si tienen un significado similar.\n",
    "\n",
    "### SVD\n",
    "\n",
    "SVD es una manera de reducir la dimensionalidad del corpus de texto que, a diferencia de lo que hemos hecho hasta ahora, no consiste en remover elementos (stopwords, etc), sino en encontrar combinaciones de palabras que resulten informativas y quedarnos con \"las mejores\" de éstas.\n",
    "\n",
    "Una analogía posible es la siguiente: podemos describir un rectángulo dando dos variables (features): su base y su altura. Si quisiéramos describir el rectángulo con una sola de estas features, por ejemplo la base, estaríamos perdiendo información muy relevante ya que existen rectángulos muy diferentes con la misma base. Sin embargo, si generáramos una nueva variable \"área\" igual al producto de base por altura, y describiéramos al rectángulo usando solamente el área, estaríamos reduciendo la dimensionalidad de una manera mucho más razonable, guardando más información sobre el rectángulo original que al quedarnos solo con la base.\n",
    "\n",
    "SVD es una transformación algebraica parecida a PCA (principal component analysis) que se puede usar en el contexto de text mining para encontrar combinaciones lineales de los términos que resulten informativas, de modo que podamos describir el data set con un número de combinaciones menor al número de términos que teníamos originalmente. De hecho, estas combinaciones pueden considerarse como dimensiones con sentido semántico latente (latent semantic dimensions), es decir, dimensiones en las que tiene sentido proyectar el dataset precisamente por su contenido semántico.\n",
    "\n",
    "El motivo por el cual podemos reducir la dimensionalidad de los textos proyectandolos a estas latent semantic dimensions es que muchas veces existe redundancia en el conjunto de documentos. Es decir que con palabras más o menos distintas, muchos documentos hablan de los mismos temas. En el ejemplo de los 6 textos que venimos usando\n",
    "\n",
    "t0='El potro y el angel llegaron al cine por casualidad.'\n",
    "t1= 'El ángel, el tanque del cine nacional, un paso más cerca del oscar',\n",
    "t2= \"final del mes del cine nacional: 'El Potro', la única cinta 'millonaria'\",\n",
    "t3= 'Juan Martin del potro volvió a tandil: se dio el ultimo baño de masas con los suyos.',\n",
    "t4= 'Juan Martin del potro fue recibido por una multitud en Tandil.',\n",
    "t5= \"Juan Martin del potro fue a ver 'El Potro' al cine y le encantó.\"\n",
    "\n",
    "\n",
    "hay 45 palabras distintas. (Antes redujimos el número de términos a 28 quitando stopwords). Sin embargo los textos hablan esencialmente de tres temas: hay dos películas nuevas en el cine, Del Potro visitó Tandil, Del Potro fue al cine.\n",
    "\n",
    "Esta reducción de la dimensionalidad podría mejorar la performance de un clasificador o un modelo de clustering. Por otro lado, una reducción a dos o tres dimensiones nos puede permitir visualizar los datos. Hay que tener en cuenta, sin embargo, que en general necesitaremos más dimensiones para describir correctamente el corpus y es posible que la representación en dos dimensiones no nos revele mucho sobre la estructura del dataset.\n",
    "\n",
    "Lo anterior pretende dar una intuición sobre la descomposición SVD y su aplicación al análisis de textos. Comprender la matemática involucrada excede las ambiciones de esta presentación. Para indagar en ella pueden consultar alguna de estas referencias:\n",
    "\n",
    "Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications 2012, Chapter 11.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Singular_value_decomposition\n",
    "\n",
    "\n",
    "## LDA\n",
    "\n",
    "LDA es una técnica de modelado de tópicos ampliamente difundida, desarrollada en 2003 por Blei, Ng y Jordan. Dado un corpus, como resultado de aplicar LDA obtendremos 2 elementos:\n",
    "\n",
    "Una distribución de tópicos sobre cada documento.\n",
    "Una distribución de las palabras sobre los tópicos.\n",
    "\n",
    "De esta forma, podemos tomar las palabras más importantes de cada tópico para definirlo, a la vez que decimos que cada documento habla de los tópicos que más pesan en su distribución.\n",
    "\n",
    "También podemos ver cuáles son los tópicos más importantes de corpus, y en un tema más avanzado (dynamic topic modeling), cómo evolucionan los tópicos en el tiempo.\n",
    "\n",
    "LDA es un modelo de aprendizaje no supervisado. Tenemos que evaluar los resultados del modelo de acuerdo a los  objetivos de nuestro problema y a la razonabilidad de los resultados. \n",
    "Es un proceso iterativo, durante el cual entrenamos el modelo, evaluamos los resultados, hacemos ajustes y lo volvemos a entrenar.\n",
    "Tenemos que predefinir la cantidad de tópicos. Son un hiperparámetro del modelo. Pueden usarse técnicas de clustering para evaluar los resultados.\n",
    "Los tópicos deben ser interpretados por el analista de acuerdo a la distribución de palabras que arroja el modelo. \n",
    "En ocasiones, puede ser útil agregar a las stop words palabras comunes a todos los documentos que no nos ayudan a separar los tópicos. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
