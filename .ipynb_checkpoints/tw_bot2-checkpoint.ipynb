{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import logging\n",
    "import time\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "with open('since.p', 'rb') as pfile:\n",
    "    since_id = pickle.load(pfile)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "consumer_key = \"EtKmd1NbqzXAZAIdWpb7xbRKg\"\n",
    "consumer_secret = \"R9tSQTZOv9wuDqis8UIhmLJ2jFUtATDYrTtUlTsb9UdGQk7GuJ\"\n",
    "access_token =\"1323692400158625792-Ve3TDs7u9kIoBiBYwLzE7oU7hjVDXj\"\n",
    "access_token_secret =\"kPe9lFls2BDokKMAGh2NdNGssXYWinN9iUlxVwQDN5iVC\"\n",
    "\n",
    "def tw_api():\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    try:\n",
    "        api.verify_credentials()\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error creating API\", exc_info=True)\n",
    "        raise e\n",
    "    return api\n",
    "\n",
    "\n",
    "def mentions(api,since_id):\n",
    "    since_id = since_id\n",
    "    cursor=tweepy.Cursor(api.mentions_timeline,since_id=since_id,tweet_mode=\"extended\").items()\n",
    "    tuis=[]\n",
    "    for i,x in enumerate(cursor):\n",
    "        if x.in_reply_to_user_id_str != '1323692400158625792': \n",
    "            tuis.append(x)        \n",
    "            since_id = max(x.id, since_id)\n",
    "    print(len(tuis))\n",
    "    return(tuis,since_id)\n",
    "\n",
    "def response(tuits):\n",
    "    for tweet in tuits:\n",
    "        if tweet.in_reply_to_status_id is not None:  \n",
    "            if tweet.in_reply_to_user_id_str != '1323692400158625792':                \n",
    "                fdic[tweet.full_text.split(' ')[0]](tweet)\n",
    "                since_id = max(tweet.id, since_id)\n",
    "    with open('since.p', 'wb') as pfile:\n",
    "        pickle.dump(since_id, pfile)             \n",
    "    return()\n",
    "                \n",
    "def post(txt,tweet):\n",
    "    resus=[txt[i:i+n] for i in range(0, len(txt), n)]\n",
    "    for j,resu in enumerate(resus):    \n",
    "\n",
    "        if j == 0 :\n",
    "\n",
    "            try:\n",
    "                resp=api.update_status(status=\"@\"+tweet.author.screen_name+' '+resu,in_reply_to_status_id=tweet.id)\n",
    "            except:\n",
    "                logger.info(f\"fail to post\")\n",
    "            time.sleep(1)  \n",
    "        else  :\n",
    "            try:\n",
    "                resp=api.update_status(status=\"@dr_toolong\"+' '+resu,in_reply_to_status_id=resp.id)\n",
    "            except:\n",
    "                logger.info(f\"fail to post\")\n",
    "            time.sleep(1)\n",
    "    return()\n",
    "                            \n",
    "def goclarin(tweet):    \n",
    "    tuit0=api.get_status(tweet.in_reply_to_status_id,tweet_mode=\"extended\")\n",
    "    logger.info(f\"a reply to {'https://twitter.com/twitter/statuses/'+tuit0.id_str}\")\n",
    "    url=tuit0.entities['urls'][0]['url']\n",
    "    logger.info(f\"found url: {url}\")    \n",
    "    nota=clarin()\n",
    "    try:\n",
    "        nota.get(url)\n",
    "        n=240\n",
    "        if 'q' in tweet.full_text:\n",
    "            txt=nota.quotes\n",
    "        else:\n",
    "            txt=' / '.join(nota.bolds)\n",
    "        post(txt,tweet)               \n",
    "    except:\n",
    "        logger.info(f\"cant scrap\")                  \n",
    "\n",
    "    \n",
    "def gocronica(tweet):  \n",
    "    tuit0=api.get_status(tweet.in_reply_to_status_id,tweet_mode=\"extended\")\n",
    "    url=tuit0.entities['urls'][0]['url']\n",
    "    logger.info(f\"found url: {url}\")\n",
    "    nota=cronica()\n",
    "    try:\n",
    "        nota.get(url)\n",
    "        n=240\n",
    "        if 'q' in tweet.full_text:\n",
    "            #quotes\n",
    "            txt=nota.quotes\n",
    "        else:\n",
    "            #bolds\n",
    "            txt=' / '.join(nota.bolds)\n",
    "        post(txt,tweet)                         \n",
    "    except:\n",
    "        logger.info(f\"cant scrap\")\n",
    "\n",
    "fdic={'@clarincom' : goclarin , '@cronica' : gocronica}\n",
    "\n",
    "class clarin():\n",
    "    def __init__(self):\n",
    "        self.url='https://www.clarin.com'\n",
    "    \n",
    "    def get(self,url):\n",
    "\n",
    "        nota=r.get(url)\n",
    "        sopa=bs(nota.content,features=\"lxml\")\n",
    "        ps=sopa.find('div','body-nota').findAll('p')\n",
    "        st=sopa.find('div','body-nota').findAll('strong')\n",
    "        self.volanta=sopa.find('p','volanta').text\n",
    "        self.titulo=sopa.find('h1').text\n",
    "        self.bajada=sopa.find('div','bajada').find('h2').text\n",
    "        texto=list()\n",
    "        for p in ps:\n",
    "            if p.text == \"COMENTARIOS\":\n",
    "                break\n",
    "            texto.append(p.text)\n",
    "        bolds=list()    \n",
    "        for b in st:\n",
    "            bolds.append(b.text)            \n",
    "        self.cuerpo=' '.join(texto)\n",
    "        self.quotes=' / '.join(self.cuerpo.split('\"')[1::2])\n",
    "        self.bold=' '.join(bolds)\n",
    "        self.bolds=bolds\n",
    "  \n",
    "        \n",
    "        \n",
    "class cronica():\n",
    "    def __init__(self):\n",
    "        self.url='https://www.cronica.com.ar/'\n",
    "   \n",
    "    def get(self,url):        \n",
    "        nota=r.get(url)\n",
    "        sopa=bs(nota.content,features=\"lxml\")\n",
    "        self.volanta=None\n",
    "        self.titulo=sopa.find('h1').text\n",
    "        self.bajada=sopa.find('div',{'class' : 'title'}).text\n",
    "        bolds=[x.text for x in sopa.find('div', { 'class' :\"entry-body text-font\"}).findAll('strong')]           \n",
    "        self.bold=' / '.join(bolds)\n",
    "        self.bolds=bolds    \n",
    "        self.cuerpo=sopa.find('div', { 'class' :\"entry-body text-font\"}).text\n",
    "        self.quotes=' / '.join(self.cuerpo.split('\"')[1::2])                            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331315346691797000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "since_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=tw_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tuits,since=mentions(api,since_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response(tuits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
